{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h29YdIcZkBQ",
        "outputId": "486ddc25-3af7-4dbd-acc4-329300dd6b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cleaning up /content/via_dataset_final_processed...\n",
            "Loading metadata from: /content/drive/MyDrive/ML Project/labels_combine.csv\n",
            "Train Patients: 244 | Test Patients: 62\n",
            "Moving original files...\n",
            "Originals in Train: 235\n",
            "\n",
            "Starting Aggressive Augmentation...\n",
            "------------------------------\n",
            "Augmentation Stats:\n",
            "Originals: 235\n",
            "New Augmentations: 2685\n",
            "TOTAL Training Images: 2920\n",
            "------------------------------\n",
            "Dataset successfully overwritten at: /content/drive/MyDrive/ML Project/Combined_Dataset_Processed.zip\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# NOTEBOOK 1: BALANCED PREPROCESSING (High Volume Augmentation)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Setup Paths\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "CSV_PATH = \"/content/drive/MyDrive/ML Project/labels_combine.csv\"\n",
        "SOURCE_IMG_DIR = \"/content/drive/MyDrive/ML Project/via_dataset\"\n",
        "\n",
        "BASE_DIR = '/content/via_dataset_final_processed'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
        "TEST_DIR = os.path.join(BASE_DIR, 'test')\n",
        "# ---------------------\n",
        "\n",
        "# 2. COMPLETE CLEANUP\n",
        "print(f\"Cleaning up {BASE_DIR}...\")\n",
        "if os.path.exists(BASE_DIR):\n",
        "    shutil.rmtree(BASE_DIR)\n",
        "os.makedirs(TRAIN_DIR)\n",
        "os.makedirs(TEST_DIR)\n",
        "\n",
        "# 3. Load Data\n",
        "print(f\"Loading metadata from: {CSV_PATH}\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Clean Labels to ensure matching works\n",
        "df['label'] = df['label'].astype(str).str.strip().str.capitalize()\n",
        "\n",
        "# 4. Patient Split\n",
        "def get_patient_id(filename):\n",
        "    filename = str(filename).strip()\n",
        "    if \"IARC\" in filename:\n",
        "        match = re.search(r\"([0-9]+[A-Z]+|[A-Z]+[0-9]+)\", filename)\n",
        "        if match: return match.group(0)\n",
        "    if \"sample\" in filename:\n",
        "        return filename.split('_')[0]\n",
        "    return filename\n",
        "\n",
        "df['patient_id'] = df['filename'].apply(get_patient_id)\n",
        "unique_patients = df['patient_id'].unique()\n",
        "\n",
        "train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)\n",
        "print(f\"Train Patients: {len(train_patients)} | Test Patients: {len(test_patients)}\")\n",
        "\n",
        "# 5. Move Originals\n",
        "filename_to_label = dict(zip(df['filename'], df['label']))\n",
        "\n",
        "def move_files(patient_list, destination):\n",
        "    count = 0\n",
        "    subset = df[df['patient_id'].isin(patient_list)]\n",
        "    for _, row in subset.iterrows():\n",
        "        fname = row['filename']\n",
        "        src = os.path.join(SOURCE_IMG_DIR, fname)\n",
        "        dst = os.path.join(destination, fname)\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy2(src, dst)\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "print(\"Moving original files...\")\n",
        "n_train = move_files(train_patients, TRAIN_DIR)\n",
        "n_test = move_files(test_patients, TEST_DIR)\n",
        "print(f\"Originals in Train: {n_train}\")\n",
        "\n",
        "# 6. AGGRESSIVE AUGMENTATION LOGIC\n",
        "def generate_random_variant(image):\n",
        "    h, w = image.shape[:2]\n",
        "    aug_img = image.copy()\n",
        "\n",
        "    # 1. Rotation (Higher range)\n",
        "    angle = random.uniform(-45, 45) # Increased from 15 to 45 for variety\n",
        "    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
        "    aug_img = cv2.warpAffine(aug_img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
        "\n",
        "    # 2. Flips\n",
        "    if random.random() > 0.5: aug_img = cv2.flip(aug_img, 1)\n",
        "    if random.random() > 0.5: aug_img = cv2.flip(aug_img, 0)\n",
        "\n",
        "    # 3. Color/Contrast\n",
        "    if random.random() > 0.3: # Increased probability\n",
        "        alpha = random.uniform(0.8, 1.2)\n",
        "        beta = random.uniform(-20, 20)\n",
        "        aug_img = cv2.convertScaleAbs(aug_img, alpha=alpha, beta=beta)\n",
        "\n",
        "    return aug_img\n",
        "\n",
        "print(\"\\nStarting Aggressive Augmentation...\")\n",
        "train_files = [f for f in os.listdir(TRAIN_DIR) if f.lower().endswith('.jpg')]\n",
        "\n",
        "augmented_count = 0\n",
        "\n",
        "for fname in train_files:\n",
        "    img_path = os.path.join(TRAIN_DIR, fname)\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None: continue\n",
        "\n",
        "    label = filename_to_label.get(fname, \"Unknown\")\n",
        "\n",
        "    # --- UPDATED MULTIPLIERS ---\n",
        "    if label == 'Cancerous':\n",
        "        num_copies = 30  # 30x multiplier!\n",
        "    elif label == 'Precancerous':\n",
        "        num_copies = 15  # 15x multiplier\n",
        "    elif label == 'Normal':\n",
        "        num_copies = 5   # 5x multiplier\n",
        "    else:\n",
        "        num_copies = 0\n",
        "\n",
        "    base_name = os.path.splitext(fname)[0]\n",
        "\n",
        "    for i in range(num_copies):\n",
        "        aug_variant = generate_random_variant(image)\n",
        "        save_name = f\"{base_name}_aug_{i}.jpg\"\n",
        "        save_path = os.path.join(TRAIN_DIR, save_name)\n",
        "        cv2.imwrite(save_path, aug_variant)\n",
        "        augmented_count += 1\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Augmentation Stats:\")\n",
        "print(f\"Originals: {n_train}\")\n",
        "print(f\"New Augmentations: {augmented_count}\")\n",
        "print(f\"TOTAL Training Images: {len(os.listdir(TRAIN_DIR))}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 7. Zip and Overwrite\n",
        "zip_name = '/content/drive/MyDrive/ML Project/Combined_Dataset_Processed'\n",
        "shutil.make_archive(zip_name, 'zip', BASE_DIR)\n",
        "print(f\"Dataset successfully overwritten at: {zip_name}.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "diio5vnHZmGX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}